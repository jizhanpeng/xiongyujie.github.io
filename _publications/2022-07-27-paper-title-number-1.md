---
title: "Learning Transferable Feature Representation with Swin Transformer for Object Recognition"
collection: publications
category: manuscripts
permalink: /publication/2022-07-27-paper-title-number-1
excerpt: '<div style="text-align: justify;">Deep learning in computer vision is limited by data - scale dependence. This paper uses Swin Transformer with fine - tuning to overcome data shortage, showing good small - scale dataset object - recognition performance.</div>'
date: 2022-07-27
venue: 'Neural Processing Letters '
paperurl: 'http://jizhanpeng.cn/xiongyujie.github.io/files/2C2S_A_two-channel_and_two-stream_transformer_based_framework_for_offline_signature_verification.pdf'
citation: '<br/><div style="text-align: justify;">Learning Transferable Feature Representation with Swin Transformer for Object Recognition, J.-X. Ren, Y.-J. Xiong*, X.-J. Xie and Y.-F. Dai, Neural Processing Letters, 2023, 55 (1): 2211â€“2223</div>'
---

<div style="text-align: justify;">Recent, substantial advancements in deep learning technologies have driven the flourishing of computer vision. However, the heavy dependence on the scale of training data limits deep learning applications because it is generally hard to obtain such a large number of data in many practical scenarios. And, deep learning seems to offer no significant advantage compared with traditional machine methods in a lack of sufficient training data. The proposed approach in this paper overcomes the problem of insufficient training data by taking Swin Transformer as the backbone for feature extraction and performing the fine-tuning strategies on the target dataset for learning transferable feature representation. Our experimental results demonstrate that the proposed method has a good performance for object recognition on small-scale datasets.</div>

<br/>
